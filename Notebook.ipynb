{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ff6ed4",
   "metadata": {},
   "source": [
    "# Information Retrieval Project\n",
    "\n",
    "**Student Name:** Aparnaa Mahalaxmi Arulljothi  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "1. **Web crawler** – Scrapy crawler to collect HTML documents (demo corpus)\n",
    "2. **Document indexer** – TF-IDF based indexing using scikit-learn\n",
    "3. **Query processor** – Ranked retrieval using cosine similarity\n",
    "\n",
    "### Technologies used\n",
    "\n",
    "- **Python 3**\n",
    "- **Scrapy** – web crawling\n",
    "- **BeautifulSoup4** – HTML parsing\n",
    "- **scikit-learn** – TF-IDF vectorization\n",
    "- **Flask** – REST API framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f16ec",
   "metadata": {},
   "source": [
    "# Information Retrieval Search Engine Project\n",
    "\n",
    "- Wikipedia crawl + indexing \n",
    "- Official 3-document TF-IDF search engine \n",
    "- Query processing to generate `results.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee055ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d269d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ready: data/demo_corpus\n",
      "Directory ready: data/html_corpus\n",
      "Directory ready: data/output\n"
     ]
    }
   ],
   "source": [
    "# Creating directories for the project\n",
    "directories = [\n",
    "    'data/demo_corpus',      # wikipedia crawl\n",
    "    'data/html_corpus',      # Fo 3 HTML files \n",
    "    'data/output'            # For wiki_index.json, index.json and results.csv\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory ready: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7fa6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Web Crawler \n",
    "\n",
    "The crawler is implemented using **Scrapy**.\n",
    "\n",
    "### Crawler Specifications:\n",
    "- **https://en.wikipedia.org/wiki/Information_retrieval** \n",
    "- **Depth Limit:** 2 levels\n",
    "- **Page Limit:** 100 pages\n",
    "- **Output:** HTML files saved to `data/demo_corpus/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d66838",
   "metadata": {},
   "source": [
    "### HTML text extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf8dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html_file_path):\n",
    "  \n",
    "    \"\"\"Extract clean text from HTML file.\"\"\"\n",
    "    try:\n",
    "        with open(html_file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            html_content = file.read()       \n",
    "        soup = BeautifulSoup(html_content, 'lxml')   # Parsing HTML with BeautifulSoup\n",
    "        \n",
    "\n",
    "        for script in soup(['script', 'style', 'meta', 'link']):\n",
    "            script.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=' ')  # Clean the text\n",
    "        \n",
    "        # Removing extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text \n",
    "    except Exception as e:\n",
    "        print(f\"Error{html_file_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d004fc",
   "metadata": {},
   "source": [
    "## Wikipedia Crawler – Collects up to 100 pages from the Information Retrieval Wikipedia branch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25742eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy crawler for collecting up to 100 Wikipedia pages on Information Retrieval\n",
    "import scrapy\n",
    "from pathlib import Path\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class WikipediaIRSpider(scrapy.Spider):\n",
    "    name = \"wikipedia_ir\"\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Information_retrieval\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DEPTH_LIMIT\": 2,\n",
    "        \"CLOSESPIDER_PAGECOUNT\": 100,\n",
    "        \"ROBOTSTXT_OBEY\": True,\n",
    "        \"DOWNLOAD_DELAY\": 1.0,\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "        \"AUTOTHROTTLE_START_DELAY\": 1.0,\n",
    "        \"AUTOTHROTTLE_MAX_DELAY\": 5.0,\n",
    "        \"AUTOTHROTTLE_TARGET_CONCURRENCY\": 1.0,\n",
    "        \"LOG_LEVEL\": \"INFO\",\n",
    "        \"USER_AGENT\": \"IRCourseCrawler/1.0 (student project)\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_dir = Path(\"data/demo_corpus\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.page_counter = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Save using unique incremental filenames\n",
    "        self.page_counter += 1\n",
    "        page_id = f\"page_{self.page_counter:03d}\"\n",
    "\n",
    "        filename = self.output_dir / f\"{page_id}.html\"\n",
    "        filename.write_bytes(response.body)\n",
    "        self.logger.info(f\"Saved: {filename} ({response.url})\")\n",
    "\n",
    "        # Follow other Wikipedia links\n",
    "        for href in response.css(\"a::attr(href)\").getall():\n",
    "            if href.startswith(\"/wiki/\") and not any(prefix in href for prefix in [\":\", \"#\"]):\n",
    "                yield scrapy.Request(response.urljoin(href), callback=self.parse)\n",
    "\n",
    "\n",
    "def run_wiki_crawler():\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(WikipediaIRSpider)\n",
    "    process.start()\n",
    "\n",
    "# run_wiki_crawler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9e7b7",
   "metadata": {},
   "source": [
    "Sample print of wikipedia crawled page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c317def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total HTML files physically in demo_corpus: 100\n",
      "Number of files we will index for Wikipedia corpus: 100\n",
      "\n",
      "First 10 files used for indexing:\n",
      "  page_001.html\n",
      "  page_002.html\n",
      "  page_003.html\n",
      "  page_004.html\n",
      "  page_005.html\n",
      "  page_006.html\n",
      "  page_007.html\n",
      "  page_008.html\n",
      "  page_009.html\n",
      "  page_010.html\n",
      "\n",
      "Preview of first HTML page:\n",
      "File: page_001.html\n",
      "\n",
      "Extracted Text (first 500 characters):\n",
      "Information retrieval - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Contents move to sidebar hide (Top) 1 Overview 2 History 3 Applications Toggle Applications subsection 3.1 General applications 3.2 Doma ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wiki_corpus_path = Path(\"data/demo_corpus\")\n",
    "wiki_html_files_all = sorted(wiki_corpus_path.glob(\"*.html\"))\n",
    "\n",
    "print(f\"Total HTML files physically in demo_corpus: {len(wiki_html_files_all)}\")\n",
    "\n",
    "# Use only the first 100 for indexing (professor's max constraint)\n",
    "wiki_html_files = wiki_html_files_all[:100]\n",
    "\n",
    "print(f\"Number of files we will index for Wikipedia corpus: {len(wiki_html_files)}\")\n",
    "print(\"\\nFirst 10 files used for indexing:\")\n",
    "for f in wiki_html_files[:10]:\n",
    "    print(\" \", f.name)\n",
    "\n",
    "# Preview of the first HTML document \n",
    "if wiki_html_files:\n",
    "    first_file = wiki_html_files[0]\n",
    "    print(\"\\nPreview of first HTML page:\")\n",
    "    print(\"File:\", first_file.name)\n",
    "    sample_text = extract_text_from_html(first_file)\n",
    "    \n",
    "    print(\"\\nExtracted Text (first 500 characters):\")\n",
    "    print(sample_text[:500], \"...\")\n",
    "else:\n",
    "    print(\"No HTML files found in the folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770ac2",
   "metadata": {},
   "source": [
    "Load and preview cleaned text from the crawled Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "578e2519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building document dictionary for Wikipedia corpus\n",
      "Total Wikipedia documents loaded for indexing: 100\n",
      "\n",
      "Document ID: page_001\n",
      " Text length: 38673 characters\n",
      " Preview: Information retrieval - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Cr\n",
      "\n",
      "Document ID: page_002\n",
      " Text length: 11951 characters\n",
      " Preview: Wikipedia, the free encyclopedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Cre\n",
      "\n",
      "Document ID: page_003\n",
      " Text length: 55352 characters\n",
      " Preview: Set (mathematics) - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create\n"
     ]
    }
   ],
   "source": [
    "# Build document dictionary for crawled Wikipedia corpus\n",
    "\n",
    "wiki_documents = {}\n",
    "\n",
    "print(\"\\nBuilding document dictionary for Wikipedia corpus\")\n",
    "for html_file in wiki_html_files:\n",
    "    doc_id = html_file.stem  # e.g., page_001, page_002, ...\n",
    "    text = extract_text_from_html(html_file)  # uses your existing preprocessing\n",
    "    wiki_documents[doc_id] = text\n",
    "\n",
    "print(f\"Total Wikipedia documents loaded for indexing: {len(wiki_documents)}\")\n",
    "\n",
    "# Show a few sample docs as artifacts\n",
    "for i, (doc_id, text) in enumerate(wiki_documents.items()):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"\\nDocument ID: {doc_id}\")\n",
    "    print(f\" Text length: {len(text)} characters\")\n",
    "    print(f\" Preview: {text[:300]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1a569",
   "metadata": {},
   "source": [
    "Build the TF-IDF index for the 100-page Wikipedia corpus and save it as wiki_index.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830bb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building TF-IDF index for Wikipedia corpus\n",
      "Documents: 100\n",
      "TF-IDF matrix created for Wikipedia corpus\n",
      "Documents: 100\n",
      "Vocabulary size: 24580 terms\n",
      "Matrix shape: (100, 24580)\n",
      "Sparsity: 95.96%\n",
      "\n",
      "Wikipedia index saved\n",
      "Location: data\\output\\wiki_index.json\n",
      "File size: 30844.18 KB\n",
      "Index contains:\n",
      " - 100 documents\n",
      " - 24580 vocabulary terms\n",
      " - TF-IDF matrix: 100 x 24580\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF index for the Wikipedia corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nBuilding TF-IDF index for Wikipedia corpus\")\n",
    "\n",
    "wiki_doc_ids = list(wiki_documents.keys())\n",
    "wiki_texts = [wiki_documents[doc_id] for doc_id in wiki_doc_ids]\n",
    "\n",
    "print(f\"Documents: {len(wiki_doc_ids)}\")\n",
    "\n",
    "wiki_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=None,\n",
    "    norm='l2'\n",
    ")\n",
    "\n",
    "wiki_tfidf_matrix = wiki_vectorizer.fit_transform(wiki_texts)\n",
    "wiki_feature_names = wiki_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF matrix created for Wikipedia corpus\")\n",
    "print(f\"Documents: {wiki_tfidf_matrix.shape[0]}\")\n",
    "print(f\"Vocabulary size: {wiki_tfidf_matrix.shape[1]} terms\")\n",
    "print(f\"Matrix shape: {wiki_tfidf_matrix.shape}\")\n",
    "\n",
    "sparsity = 1 - wiki_tfidf_matrix.nnz / (wiki_tfidf_matrix.shape[0] * wiki_tfidf_matrix.shape[1])\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n",
    "\n",
    "# Pack index in a JSON-friendly structure\n",
    "wiki_index_data = {\n",
    "    \"document_ids\": wiki_doc_ids,\n",
    "    \"vocabulary\": wiki_feature_names.tolist(),\n",
    "    \"tfidf_matrix\": wiki_tfidf_matrix.toarray().tolist(),\n",
    "    \"vectorizer_params\": {\n",
    "        \"lowercase\": True,\n",
    "        \"stop_words\": \"english\",\n",
    "        \"norm\": \"l2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "wiki_index_path = Path(\"data/output/wiki_index.json\")\n",
    "wiki_index_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(wiki_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(wiki_index_data, f, indent=2)\n",
    "\n",
    "print(\"\\nWikipedia index saved\")\n",
    "print(f\"Location: {wiki_index_path}\")\n",
    "print(f\"File size: {os.path.getsize(wiki_index_path) / 1024:.2f} KB\")\n",
    "print(f\"Index contains:\")\n",
    "print(f\" - {len(wiki_index_data['document_ids'])} documents\")\n",
    "print(f\" - {len(wiki_index_data['vocabulary'])} vocabulary terms\")\n",
    "print(\n",
    "    f\" - TF-IDF matrix: \"\n",
    "    f\"{len(wiki_index_data['tfidf_matrix'])} x {len(wiki_index_data['tfidf_matrix'][0])}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2d6e1",
   "metadata": {},
   "source": [
    "Load the saved Wikipedia TF-IDF index and display a few sample entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ef9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia TF-IDF index from: data\\output\\wiki_index.json\n",
      "\n",
      "Index Summary\n",
      "Documents: 100\n",
      "Vocabulary size: 24580\n",
      "TF-IDF matrix shape: 100 x 24580\n",
      "\n",
      "Sample document IDs:\n",
      " - page_001\n",
      " - page_002\n",
      " - page_003\n",
      " - page_004\n",
      " - page_005\n",
      "\n",
      "Sample vocabulary terms:\n",
      " - 00\n",
      " - 000\n",
      " - 000000000103\n",
      " - 00001\n",
      " - 00009\n",
      " - 0001\n",
      " - 00019\n",
      " - 0002\n",
      " - 0003\n",
      " - 00037\n",
      " - 00051\n",
      " - 0005858401960206\n",
      " - 0006\n",
      " - 00062\n",
      " - 00065\n",
      "\n",
      "TF-IDF vector sample for the first document:\n",
      "Length: 24580\n",
      "First 20 values: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#few sample entries from the Wikipedia index JSON\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "wiki_index_path = Path(\"data/output/wiki_index.json\")\n",
    "print(\"Loading Wikipedia TF-IDF index from:\", wiki_index_path)\n",
    "with open(wiki_index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki_index = json.load(f)\n",
    "print(\"\\nIndex Summary\")\n",
    "print(\"Documents:\", len(wiki_index[\"document_ids\"]))\n",
    "print(\"Vocabulary size:\", len(wiki_index[\"vocabulary\"]))\n",
    "print(\"TF-IDF matrix shape:\", len(wiki_index[\"tfidf_matrix\"]), \"x\", len(wiki_index[\"tfidf_matrix\"][0]))\n",
    "\n",
    "#couple of document IDs as a sample\n",
    "print(\"\\nSample document IDs:\")\n",
    "for doc_id in wiki_index[\"document_ids\"][:5]:\n",
    "    print(\" -\", doc_id)\n",
    "\n",
    "#few vocabulary terms\n",
    "print(\"\\nSample vocabulary terms:\")\n",
    "for term in wiki_index[\"vocabulary\"][:15]:\n",
    "    print(\" -\", term)\n",
    "print(\"\\nTF-IDF vector sample for the first document:\")\n",
    "first_vec = wiki_index[\"tfidf_matrix\"][0]\n",
    "print(\"Length:\", len(first_vec))\n",
    "print(\"First 20 values:\", first_vec[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3aee0",
   "metadata": {},
   "source": [
    "## Document Indexer \n",
    "\n",
    "The indexer processes HTML documents and builds a TF-IDF based vector space model for information retrieval.\n",
    "\n",
    "### Input:\n",
    "- **3  HTML files** from `data/html_corpus/`:\n",
    "  - `0F64A61C-DF01-4F43-8B8D-F0319C41768E.html`\n",
    "  - `1F648A7F-2C64-458C-BFAF-463A071530ED.html`\n",
    "  - `6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html`\n",
    "\n",
    "### Process:\n",
    "HTML Pages ➝ HTML Parsing ➝ Clean Text ➝ TF-IDF Vectorization ➝ Document–Term Matrix ➝ index.json\n",
    "\n",
    "\n",
    "### Output:\n",
    "- index.json - Contains TF-IDF data and document information for query processing\n",
    "\n",
    "- HTML Parser - Extracts text from HTML documents\n",
    "- TF-IDF Vectorizer - Converts documents to weighted term vectors\n",
    "- Index Builder - Serializes the model to JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab3085",
   "metadata": {},
   "source": [
    "## **HTML Parsing and Clean Text Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d383f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files\n",
      "Loaded: 0F64A61C-DF01-4F43-8B8D-F0319C41768E.html\n",
      "  Text length: 58606 characters\n",
      "Loaded: 1F648A7F-2C64-458C-BFAF-463A071530ED.html\n",
      "  Text length: 80858 characters\n",
      "Loaded: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html\n",
      "  Text length: 38748 characters\n",
      "\n",
      "Total documents loaded: 3\n"
     ]
    }
   ],
   "source": [
    "official_files = [\n",
    "    '0F64A61C-DF01-4F43-8B8D-F0319C41768E.html',\n",
    "    '1F648A7F-2C64-458C-BFAF-463A071530ED.html',\n",
    "    '6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html'\n",
    "]\n",
    "corpus_path = Path('data/html_corpus')\n",
    "print(\"Checking files\")\n",
    "documents = {}\n",
    "for filename in official_files:\n",
    "    file_path = corpus_path / filename\n",
    "    if file_path.exists(): \n",
    "        doc_id = filename.replace('.html', '')   # Extract document ID from filename \n",
    "        text = extract_text_from_html(file_path)  # Extract text content\n",
    "        documents[doc_id] = text\n",
    "        print(f\"Loaded: {filename}\")\n",
    "        print(f\"  Text length: {len(text)} characters\")\n",
    "    else:\n",
    "        print(f\"Missing: {filename}\")\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434f751",
   "metadata": {},
   "source": [
    "## Build TF-IDF vectors for the three given documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57dac98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF Index\n",
      "TF-IDF matrix created\n",
      "Documents: 3\n",
      "Vocabulary size: 4692 unique terms\n",
      "Matrix shape: (3, 4692)\n",
      "Matrix sparsity: 53.71%\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF vectors for  documents\n",
    "print(\"Building TF-IDF Index\")\n",
    "doc_ids = list(documents.keys())\n",
    "doc_texts = [documents[doc_id] for doc_id in doc_ids]\n",
    "vectorizer = TfidfVectorizer(               # Create TF-IDF vectorizer\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=None,  \n",
    "    norm='l2'  # L2 normalization for cosine similarity\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(doc_texts)  # Fit_transform documents\n",
    "feature_names = vectorizer.get_feature_names_out()  # Get feature names (terms)\n",
    "print(f\"TF-IDF matrix created\")\n",
    "print(f\"Documents: {tfidf_matrix.shape[0]}\")\n",
    "print(f\"Vocabulary size: {tfidf_matrix.shape[1]} unique terms\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Matrix sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314d344",
   "metadata": {},
   "source": [
    "##  Building the Inverted Index (index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ebfd2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved successfully\n",
      "Location: data/output/index.json\n",
      "File size: 348.69 KB\n",
      "\n",
      "Index contains:\n",
      "  - 3 documents\n",
      "  - 4692 vocabulary terms\n",
      "  - TF-IDF matrix: 3 x 4692 \n"
     ]
    }
   ],
   "source": [
    "# Create the index structure \n",
    "index_data = {\n",
    "    'document_ids': doc_ids,\n",
    "    'vocabulary': feature_names.tolist(),\n",
    "    'tfidf_matrix': tfidf_matrix.toarray().tolist(),  # Convert sparse matrix to list\n",
    "    'vectorizer_params': {\n",
    "        'lowercase': True,\n",
    "        'stop_words': 'english',\n",
    "        'norm': 'l2'\n",
    "    }\n",
    "}\n",
    "\n",
    "index_file_path = 'data/output/index.json'     # Save index to JSON file\n",
    "with open(index_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(index_data, f, indent=2)\n",
    "print(\"Index saved successfully\")\n",
    "print(f\"Location: {index_file_path}\")\n",
    "print(f\"File size: {os.path.getsize(index_file_path) / 1024:.2f} KB\")\n",
    "print(f\"\\nIndex contains:\")\n",
    "print(f\"  - {len(index_data['document_ids'])} documents\")\n",
    "print(f\"  - {len(index_data['vocabulary'])} vocabulary terms\")\n",
    "print(f\"  - TF-IDF matrix: {len(index_data['tfidf_matrix'])} x {len(index_data['tfidf_matrix'][0])} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bf189",
   "metadata": {},
   "source": [
    "## Part 3: Query Processor \n",
    "\n",
    "The query processor loads the index and performs ranked retrieval using cosine similarity.\n",
    "\n",
    "### Input:\n",
    "- index.json - The TF-IDF index created above\n",
    "- queries.csv - Query file with columns: `query_id`, `query_text`\n",
    "\n",
    "### Process:\n",
    "Queries ➝ TF-IDF Vectorization ➝ Cosine Similarity Computation ➝ Document Ranking ➝ Top-K Results\n",
    "\n",
    "\n",
    "### Output:\n",
    "- results.csv - Ranked results with columns: `query_id`, `rank`, `document_id`\n",
    "\n",
    "### Ranking Method:\n",
    "- **Cosine Similarity** - Measures the angle between query and document vectors\n",
    "- Documents are ranked in descending order of similarity\n",
    "- All 3 documents are ranked for each query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb68637",
   "metadata": {},
   "source": [
    "In this step, I load the saved TF-IDF index (index.json) and reconstruct the document IDs, vocabulary, and matrix for searching. I also load the instructor-provided queries.csv file and print a quick preview to confirm everything is ready for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d2b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index and queries\n",
      "Index loaded successfully\n",
      "  Documents: 3\n",
      "  Vocabulary: 4692 terms\n",
      "\n",
      "Queries loaded successfully\n",
      "Total queries: 3\n",
      "\n",
      "Query preview:\n",
      "  - 6E93CDD1-52F9-4F41-A405-54E398EF6FF8: 'information overload'\n",
      "  - 0D97BCC6-C46E-4242-9777-7CEAED55B362: 'database server hardware specs'\n",
      "  - 78452FF4-94D7-422C-9283-A14615C44ADC: 'search engine open sorce'\n"
     ]
    }
   ],
   "source": [
    "# Load the index from JSON\n",
    "print(\"Loading index and queries\")\n",
    "with open('data/output/index.json', 'r', encoding='utf-8') as f:  # Load index.json\n",
    "    loaded_index = json.load(f)\n",
    "# Reconstruct the TF-IDF components\n",
    "loaded_doc_ids = loaded_index['document_ids']\n",
    "loaded_vocabulary = loaded_index['vocabulary']\n",
    "loaded_tfidf_matrix = np.array(loaded_index['tfidf_matrix'])\n",
    "\n",
    "print(f\"Index loaded successfully\")\n",
    "print(f\"  Documents: {len(loaded_doc_ids)}\")\n",
    "print(f\"  Vocabulary: {len(loaded_vocabulary)} terms\")\n",
    "queries = []                                           # Load queries from CSV\n",
    "with open('queries.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        queries.append({\n",
    "            'query_id': row['query_id'],\n",
    "            'query_text': row['query_text']\n",
    "        })\n",
    "print(f\"\\nQueries loaded successfully\")\n",
    "print(f\"Total queries: {len(queries)}\")\n",
    "print(\"\\nQuery preview:\")\n",
    "for q in queries:\n",
    "    print(f\"  - {q['query_id']}: '{q['query_text']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfb2ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query processing function\n"
     ]
    }
   ],
   "source": [
    "def process_query(query_text, vocabulary, tfidf_matrix, doc_ids):\n",
    "    \"\"\"Process query and return ranked documents by similarity score.\"\"\"\n",
    "    # Create a vectorizer with the same vocabulary\n",
    "    query_vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        vocabulary=vocabulary,\n",
    "        norm='l2'\n",
    "    )\n",
    "    query_vector = query_vectorizer.fit_transform([query_text]).toarray()  # Transform query to TF-IDF vector\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)[0]  # Compute cosine similarity\n",
    "    doc_scores = list(zip(doc_ids, similarities)) \n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)   # Sort by score in descending order   \n",
    "    # Add rank (1-indexed)\n",
    "    ranked_results = [(doc_id, rank + 1, score) \n",
    "                      for rank, (doc_id, score) in enumerate(doc_scores)]\n",
    "    \n",
    "    return ranked_results\n",
    "print(\"Query processing function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd99dc",
   "metadata": {},
   "source": [
    "For each query, I convert the text into a TF-IDF vector using the same vocabulary as the index. I compute cosine similarity with all documents, rank them, print the top results, and save everything into results.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da6e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing queries\n",
      "\n",
      "Query: 'information overload'\n",
      "  Results:\n",
      "    Rank 1: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3 (score: 0.3643)\n",
      "    Rank 2: 0F64A61C-DF01-4F43-8B8D-F0319C41768E (score: 0.0735)\n",
      "    Rank 3: 1F648A7F-2C64-458C-BFAF-463A071530ED (score: 0.0688)\n",
      "\n",
      "Query: 'database server hardware specs'\n",
      "  Results:\n",
      "    Rank 1: 1F648A7F-2C64-458C-BFAF-463A071530ED (score: 0.3688)\n",
      "    Rank 2: 0F64A61C-DF01-4F43-8B8D-F0319C41768E (score: 0.0225)\n",
      "    Rank 3: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3 (score: 0.0157)\n",
      "\n",
      "Query: 'search engine open sorce'\n",
      "  Results:\n",
      "    Rank 1: 0F64A61C-DF01-4F43-8B8D-F0319C41768E (score: 0.5579)\n",
      "    Rank 2: 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3 (score: 0.1255)\n",
      "    Rank 3: 1F648A7F-2C64-458C-BFAF-463A071530ED (score: 0.0308)\n",
      "Results saved to: data/output/results.csv\n",
      "Total result entries: 9\n"
     ]
    }
   ],
   "source": [
    "# Process all queries and collect results\n",
    "print(\"Processing queries\")\n",
    "all_results = []\n",
    "\n",
    "for query in queries:\n",
    "    query_id = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    \n",
    "    print(f\"\\nQuery: '{query_text}'\")\n",
    "    ranked_docs = process_query(           # Ranked results\n",
    "        query_text, \n",
    "        loaded_vocabulary, \n",
    "        loaded_tfidf_matrix, \n",
    "        loaded_doc_ids\n",
    "    )\n",
    "          \n",
    "    print(f\"  Results:\")            # Display results\n",
    "    for doc_id, rank, score in ranked_docs:\n",
    "        print(f\"    Rank {rank}: {doc_id} (score: {score:.4f})\")\n",
    "        all_results.append({\n",
    "            'query_id': query_id,\n",
    "            'rank': rank,\n",
    "            'document_id': doc_id\n",
    "        })\n",
    "# Save results to CSV\n",
    "results_file_path = 'data/output/results.csv'\n",
    "with open(results_file_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['query_id', 'rank', 'document_id'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_results)\n",
    "\n",
    "print(f\"Results saved to: {results_file_path}\")\n",
    "print(f\"Total result entries: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbdd0c",
   "metadata": {},
   "source": [
    "Display the generated results.csv with ranked outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7bde3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Results\n",
      "Query ID                                 Rank   Document ID                             \n",
      "6E93CDD1-52F9-4F41-A405-54E398EF6FF8     1      6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3    \n",
      "6E93CDD1-52F9-4F41-A405-54E398EF6FF8     2      0F64A61C-DF01-4F43-8B8D-F0319C41768E    \n",
      "6E93CDD1-52F9-4F41-A405-54E398EF6FF8     3      1F648A7F-2C64-458C-BFAF-463A071530ED    \n",
      "0D97BCC6-C46E-4242-9777-7CEAED55B362     1      1F648A7F-2C64-458C-BFAF-463A071530ED    \n",
      "0D97BCC6-C46E-4242-9777-7CEAED55B362     2      0F64A61C-DF01-4F43-8B8D-F0319C41768E    \n",
      "0D97BCC6-C46E-4242-9777-7CEAED55B362     3      6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3    \n",
      "78452FF4-94D7-422C-9283-A14615C44ADC     1      0F64A61C-DF01-4F43-8B8D-F0319C41768E    \n",
      "78452FF4-94D7-422C-9283-A14615C44ADC     2      6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3    \n",
      "78452FF4-94D7-422C-9283-A14615C44ADC     3      1F648A7F-2C64-458C-BFAF-463A071530ED    \n",
      "\n",
      "showing total of 9 rows\n"
     ]
    }
   ],
   "source": [
    "# Display the generated results.csv \n",
    "print(\"Generated Results\")\n",
    "with open('data/output/results.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    results_data = list(reader)\n",
    "\n",
    "# Display first 10 rows\n",
    "#print(\"\\nFirst 10 rows of results.csv:\")\n",
    "print(f\"{'Query ID':<40} {'Rank':<6} {'Document ID':<40}\")\n",
    "for row in results_data[:10]:\n",
    "    print(f\"{row['query_id']:<40} {row['rank']:<6} {row['document_id']:<40}\")\n",
    "\n",
    "print(f\"\\nshowing total of {len(results_data)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e61ff2",
   "metadata": {},
   "source": [
    "\n",
    "## **Sample query results and their relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9904c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Query Analysis\n",
      "\n",
      "Query ID: 6E93CDD1-52F9-4F41-A405-54E398EF6FF8\n",
      "Query Text: 'information overload'\n",
      "\n",
      "Query Terms:  ['information', 'overload']\n",
      "\n",
      "Rank   Document ID                              Score     \n",
      "1      6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3     0.364314\n",
      "2      0F64A61C-DF01-4F43-8B8D-F0319C41768E     0.073532\n",
      "3      1F648A7F-2C64-458C-BFAF-463A071530ED     0.068816\n",
      "\n",
      "Query terms found in vocabulary: ['information', 'overload']\n"
     ]
    }
   ],
   "source": [
    "# Analyze a sample query in detail\n",
    "print(\"Detailed Query Analysis\")\n",
    "# Pick the first query for detailed analysis\n",
    "sample_query = queries[0]\n",
    "query_id = sample_query['query_id']\n",
    "query_text = sample_query['query_text']\n",
    "\n",
    "print(f\"\\nQuery ID: {query_id}\")\n",
    "print(f\"Query Text: '{query_text}'\")\n",
    "print(\"\\nQuery Terms: \", query_text.lower().split())\n",
    "\n",
    "# Process the query\n",
    "ranked_docs = process_query(\n",
    "    query_text,\n",
    "    loaded_vocabulary,\n",
    "    loaded_tfidf_matrix,\n",
    "    loaded_doc_ids\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Document ID':<40} {'Score':<10}\")\n",
    "for doc_id, rank, score in ranked_docs:\n",
    "    print(f\"{rank:<6} {doc_id:<40} {score:.6f}\")\n",
    "# Show which query terms are in the vocabulary\n",
    "query_terms = [term for term in query_text.lower().split() if term in loaded_vocabulary]\n",
    "print(f\"\\nQuery terms found in vocabulary: {query_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae84d7",
   "metadata": {},
   "source": [
    "## Display detailed statistics of the indexed documents, vocabulary, TF-IDF matrix, and query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf43f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Statistics\n",
      "\n",
      "Document Statistics:\n",
      "  Total documents indexed: 3\n",
      "  Document IDs:\n",
      "    1. 0F64A61C-DF01-4F43-8B8D-F0319C41768E\n",
      "    2. 1F648A7F-2C64-458C-BFAF-463A071530ED\n",
      "    3. 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\n",
      "\n",
      "Vocabulary Statistics:\n",
      "  Total unique terms: 4692\n",
      "  Sample terms (first 20): ['00063', '00065', '0020', '004', '0086', '01', '012', '0131580183', '0131964280', '0138613372', '016555159802400509', '0167', '01972240050133634', '02', '020', '0201741308', '025', '0252087127', '0257', '0271']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "  Shape: 3 documents × 4692 terms\n",
      "  Non-zero entries: 6516\n",
      "  Sparsity: 53.71%\n",
      "\n",
      "Query Statistics:\n",
      "  Total queries processed: 3\n",
      "  Results per query: 3 (all documents ranked)\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive index statistics\n",
    "print(\"Index Statistics\")\n",
    "# Document statistics\n",
    "print(f\"\\nDocument Statistics:\")\n",
    "print(f\"  Total documents indexed: {len(loaded_doc_ids)}\")\n",
    "print(f\"  Document IDs:\")\n",
    "for i, doc_id in enumerate(loaded_doc_ids, 1):\n",
    "    print(f\"    {i}. {doc_id}\")\n",
    "\n",
    "# Vocabulary statistics\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(f\"  Total unique terms: {len(loaded_vocabulary)}\")\n",
    "print(f\"  Sample terms (first 20): {loaded_vocabulary[:20]}\")\n",
    "\n",
    "# TF-IDF matrix statistics\n",
    "print(f\"\\nTF-IDF Matrix:\")\n",
    "print(f\"  Shape: {loaded_tfidf_matrix.shape[0]} documents × {loaded_tfidf_matrix.shape[1]} terms\")\n",
    "print(f\"  Non-zero entries: {np.count_nonzero(loaded_tfidf_matrix)}\")\n",
    "print(f\"  Sparsity: {(1 - np.count_nonzero(loaded_tfidf_matrix) / loaded_tfidf_matrix.size) * 100:.2f}%\")\n",
    "\n",
    "# Query statistics\n",
    "print(f\"\\nQuery Statistics:\")\n",
    "print(f\"  Total queries processed: {len(queries)}\")\n",
    "print(f\"  Results per query: {len(loaded_doc_ids)} (all documents ranked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b1f4c",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "I implement two optional features:\n",
    "\n",
    "1. **Spelling Correction (NLTK)**\n",
    "   - Detects and corrects misspelled query terms, It uses Uses dictionary-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e685a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully\n",
      "English vocabulary loaded: 235892 words\n"
     ]
    }
   ],
   "source": [
    "# Import NLTK and download required resources\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics.distance import edit_distance\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)  \n",
    "nltk.download('words', quiet=True)    # English word list\n",
    "\n",
    "from nltk.corpus import words\n",
    "english_words = set(words.words())\n",
    "\n",
    "print(\"NLTK resources downloaded successfully\")\n",
    "print(f\"English vocabulary loaded: {len(english_words)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62430f4f",
   "metadata": {},
   "source": [
    "Spelling correction module using NLTK to fix misspelled query terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93cc1bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction Examples:\n",
      "  informaton      → information     (Corrected)\n",
      "  search          → search          (Already correct)\n",
      "  engine          → engine          (Already correct)\n",
      "  retrieval       → retrieval       (Already correct)\n"
     ]
    }
   ],
   "source": [
    "def correct_spelling(word):\n",
    "    \"\"\"Return corrected spelling or original word if correct.\"\"\"\n",
    "    word_lower = word.lower()\n",
    "      \n",
    "    if word_lower in english_words:   # If word is already correct, return it\n",
    "        return word_lower\n",
    "     \n",
    "    candidates = [w for w in english_words    # Find the closest match using edit distance\n",
    "                  if abs(len(w) - len(word_lower)) <= 2]\n",
    "    \n",
    "    if not candidates:\n",
    "        return word_lower\n",
    "        \n",
    "    closest_word = min(candidates,  # Find word with minimum edit distance\n",
    "                       key=lambda w: edit_distance(word_lower, w))\n",
    "    \n",
    "    if edit_distance(word_lower, closest_word) <= 2:\n",
    "        return closest_word\n",
    "    \n",
    "    return word_lower\n",
    "\n",
    "# Test spelling correction \n",
    "print(\"Spelling Correction Examples:\")\n",
    "test_words = ['informaton', 'search', 'engine', 'retrieval']\n",
    "for word in test_words:\n",
    "    corrected = correct_spelling(word)\n",
    "    status = \"Corrected\" if word != corrected else \"Already correct\"\n",
    "    print(f\"  {word:15} → {corrected:15} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033d4ab",
   "metadata": {},
   "source": [
    "### Semantic Search using Word2Vec Embeddings\n",
    "\n",
    "**Word2Vec:**\n",
    "- Uses pre-trained word embeddings to capture semantic relationships\n",
    "- Documents and queries are represented as dense vectors \n",
    "- Cosine similarity in embedding space finds semantically related documents\n",
    "- Model: **glove-wiki-gigaword-50** (50-dimensional vectors, lightweight)\n",
    "- Cosine similarity between query and document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae73f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word embeddings...\n",
      "Model: glove-wiki-gigaword-50 (50-dimensional vectors)\n",
      "Word2Vec model loaded successfully!\n",
      "Vocabulary size: 400000 words\n",
      "Vector dimensions: 50\n",
      "\n",
      "Example - Words similar to 'search':\n",
      "  searching: 0.8898\n",
      "  searches: 0.8053\n",
      "  information: 0.7864\n",
      "  finding: 0.7863\n",
      "  tracking: 0.7722\n"
     ]
    }
   ],
   "source": [
    "# Import gensim for word embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Loading pre-trained word embeddings...\")\n",
    "print(\"Model: glove-wiki-gigaword-50 (50-dimensional vectors)\")\n",
    "# Load the pretrained model\n",
    "word2vec_model = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "print(\"Word2Vec model loaded successfully!\")\n",
    "print(f\"Vocabulary size: {len(word2vec_model)} words\")\n",
    "print(f\"Vector dimensions: {word2vec_model.vector_size}\")\n",
    "# Test with a simple example\n",
    "test_word = \"search\"\n",
    "if test_word in word2vec_model:\n",
    "    similar_words = word2vec_model.most_similar(test_word, topn=5)\n",
    "    print(f\"\\nExample - Words similar to '{test_word}':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b5372",
   "metadata": {},
   "source": [
    "Generate document-level embeddings by averaging Word2Vec vectors for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cf5ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document embeddings using Word2Vec...\n",
      "0F64A61C-DF01-4F43-8B8D-F0319C41768E\n",
      "  Words in vocabulary: 8124/9643 (84.2%)\n",
      "1F648A7F-2C64-458C-BFAF-463A071530ED\n",
      "  Words in vocabulary: 10594/12213 (86.7%)\n",
      "6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3\n",
      "  Words in vocabulary: 4754/5726 (83.0%)\n",
      " Document embeddings created: 3 documents\n"
     ]
    }
   ],
   "source": [
    "def get_document_embedding(text, model):\n",
    "    \"\"\"Convert document text to embedding by averaging word vectors.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Filter words that exist in the model's vocabulary\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_vectors.append(model[word])\n",
    "    \n",
    "    # Return average of word vectors (or zero vector if no words found)\n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"Creating document embeddings using Word2Vec...\")\n",
    "doc_embeddings = {}\n",
    "for doc_id, text in documents.items():\n",
    "    embedding = get_document_embedding(text, word2vec_model)\n",
    "    doc_embeddings[doc_id] = embedding\n",
    "    \n",
    "    # Count how many words were found in vocabulary\n",
    "    words_in_vocab = sum(1 for word in text.lower().split() if word in word2vec_model)\n",
    "    total_words = len(text.split())\n",
    "    print(f\"{doc_id}\")\n",
    "    print(f\"  Words in vocabulary: {words_in_vocab}/{total_words} ({words_in_vocab/total_words*100:.1f}%)\")\n",
    "\n",
    "\n",
    "print(f\" Document embeddings created: {len(doc_embeddings)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86dc63fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec query processing function\n"
     ]
    }
   ],
   "source": [
    "def process_query_word2vec(query_text, doc_embeddings, model):\n",
    "    \"\"\"Rank documents using Word2Vec semantic similarity.\"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_document_embedding(query_text, model)\n",
    "    \n",
    "    # Compute cosine similarity with each document\n",
    "    similarities = []\n",
    "    for doc_id, doc_embedding in doc_embeddings.items():\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1),\n",
    "            doc_embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append((doc_id, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Add ranks\n",
    "    ranked_results = [(doc_id, rank + 1, score) \n",
    "                      for rank, (doc_id, score) in enumerate(similarities)]\n",
    "    \n",
    "    return ranked_results\n",
    "print(\"Word2Vec query processing function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2462",
   "metadata": {},
   "source": [
    "# Compare TF-IDF and Word2Vec results for all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f9d83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison: TF-IDF vs Word2Vec Semantic Search\n",
      "\n",
      "Query: 'information overload'\n",
      "TF-IDF Ranking                      | Word2Vec Ranking                   \n",
      "Rank 1: 6B3BD97C-DEF2-49BB-B... (0.3643) | Rank 1: 6B3BD97C-DEF2-49BB-B... (0.6950)\n",
      "Rank 2: 0F64A61C-DF01-4F43-8... (0.0735) | Rank 2: 1F648A7F-2C64-458C-B... (0.6930)\n",
      "Rank 3: 1F648A7F-2C64-458C-B... (0.0688) | Rank 3: 0F64A61C-DF01-4F43-8... (0.6510)\n",
      "\n",
      "Query: 'database server hardware specs'\n",
      "TF-IDF Ranking                      | Word2Vec Ranking                   \n",
      "Rank 1: 1F648A7F-2C64-458C-B... (0.3688) | Rank 1: 1F648A7F-2C64-458C-B... (0.5860)\n",
      "Rank 2: 0F64A61C-DF01-4F43-8... (0.0225) | Rank 2: 6B3BD97C-DEF2-49BB-B... (0.5558)\n",
      "Rank 3: 6B3BD97C-DEF2-49BB-B... (0.0157) | Rank 3: 0F64A61C-DF01-4F43-8... (0.5157)\n",
      "\n",
      "Query: 'search engine open sorce'\n",
      "TF-IDF Ranking                      | Word2Vec Ranking                   \n",
      "Rank 1: 0F64A61C-DF01-4F43-8... (0.5579) | Rank 1: 0F64A61C-DF01-4F43-8... (0.7345)\n",
      "Rank 2: 6B3BD97C-DEF2-49BB-B... (0.1255) | Rank 2: 1F648A7F-2C64-458C-B... (0.7324)\n",
      "Rank 3: 1F648A7F-2C64-458C-B... (0.0308) | Rank 3: 6B3BD97C-DEF2-49BB-B... (0.7154)\n"
     ]
    }
   ],
   "source": [
    "# Compare TF-IDF and Word2Vec results for all queries\n",
    "print(\"Comparison: TF-IDF vs Word2Vec Semantic Search\")\n",
    "for query in queries:\n",
    "    query_id = query['query_id']\n",
    "    query_text = query['query_text']\n",
    "    \n",
    "    print(f\"\\nQuery: '{query_text}'\")\n",
    "    # Get TF-IDF results\n",
    "    tfidf_results = process_query(\n",
    "        query_text,\n",
    "        loaded_vocabulary,\n",
    "        loaded_tfidf_matrix,\n",
    "        loaded_doc_ids\n",
    "    )\n",
    "    \n",
    "    # Get Word2Vec results\n",
    "    w2v_results = process_query_word2vec(\n",
    "        query_text,\n",
    "        doc_embeddings,\n",
    "        word2vec_model\n",
    "    )\n",
    "    \n",
    "    # Display side-by-side comparison\n",
    "    print(f\"{'TF-IDF Ranking':<35} | {'Word2Vec Ranking':<35}\")\n",
    "    for i in range(len(tfidf_results)):\n",
    "        tfidf_doc, tfidf_rank, tfidf_score = tfidf_results[i]\n",
    "        w2v_doc, w2v_rank, w2v_score = w2v_results[i]\n",
    "        \n",
    "        tfidf_str = f\"Rank {tfidf_rank}: {tfidf_doc[:20]}... ({tfidf_score:.4f})\"\n",
    "        w2v_str = f\"Rank {w2v_rank}: {w2v_doc[:20]}... ({w2v_score:.4f})\"\n",
    "        \n",
    "        print(f\"{tfidf_str:<35} | {w2v_str:<35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88caaf84",
   "metadata": {},
   "source": [
    "## Part 4: Flask REST API \n",
    "\n",
    "REST API for programmatic document search.\n",
    "\n",
    "**Usage:** `POST /search` with `{\"query\": \"text\", \"top_k\": 3}`\n",
    "\n",
    "Returns ranked documents with scores.\n",
    "\n",
    "**Run:** `python flask_app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbea1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask API code\n"
     ]
    }
   ],
   "source": [
    "# Flask API implementation \n",
    "app = Flask(__name__)\n",
    "api_vocabulary = None\n",
    "api_tfidf_matrix = None\n",
    "api_doc_ids = None\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search():\n",
    "    \"\"\"\n",
    "    Search endpoint that accepts a query and returns ranked results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get request data\n",
    "        data = request.get_json()\n",
    "        if not data or 'query' not in data:\n",
    "            return jsonify({'error': 'Missing query parameter'}), 400\n",
    "        query_text = data['query']\n",
    "        top_k = data.get('top_k', 3)  # Default to top 3 results\n",
    "     \n",
    "        ranked_docs = process_query(   # Process the query\n",
    "            query_text,\n",
    "            api_vocabulary,\n",
    "            api_tfidf_matrix,\n",
    "            api_doc_ids\n",
    "        )\n",
    "         \n",
    "        results = []        # Format results\n",
    "        for doc_id, rank, score in ranked_docs[:top_k]:\n",
    "            results.append({\n",
    "                'rank': rank,\n",
    "                'document_id': doc_id,\n",
    "                'score': float(score)\n",
    "            })\n",
    "        \n",
    "        return jsonify({\n",
    "            'query': query_text,\n",
    "            'results': results\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'documents': len(api_doc_ids)})\n",
    "\n",
    "def load_index_for_api():\n",
    "    \"\"\"Load the index when starting the API.\"\"\"\n",
    "    global api_vocabulary, api_tfidf_matrix, api_doc_ids\n",
    "    \n",
    "    with open('data/output/index.json', 'r', encoding='utf-8') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    api_vocabulary = index['vocabulary']\n",
    "    api_tfidf_matrix = np.array(index['tfidf_matrix'])\n",
    "    api_doc_ids = index['document_ids']\n",
    "    \n",
    "    print(f\"Index loaded: {len(api_doc_ids)} documents, {len(api_vocabulary)} terms\")\n",
    "\n",
    "# To run the API (execute separately as a Python script):\n",
    "# if __name__ == '__main__':\n",
    "#     load_index_for_api()\n",
    "#     app.run(debug=True, port=5000)\n",
    "\n",
    "print(\"Flask API code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537b27c",
   "metadata": {},
   "source": [
    "## Project Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04909f7b",
   "metadata": {},
   "source": [
    "```project/\n",
    "│\n",
    "├── api/\n",
    "│   ├── __pycache__/\n",
    "│   ├── __init__.py\n",
    "│   ├── app.py\n",
    "│   ├── static/\n",
    "│   │   ├── css/\n",
    "│   │   │   └── style.css\n",
    "│   │   └── js/\n",
    "│   │       └── script.js\n",
    "│   └── templates/\n",
    "│       └── index.html\n",
    "│\n",
    "├── crawler/\n",
    "│   ├── __pycache__/\n",
    "│   ├── __init__.py\n",
    "│   └── wiki_crawler.py\n",
    "│\n",
    "├── data/\n",
    "│   ├── demo_corpus/\n",
    "│   ├── html_corpus/\n",
    "│   │   ├── 0F64A61C-DF01-4F43-8B8D-F0319C41768E.html\n",
    "│   │   ├── 1F648A7F-2C64-458C-BFAF-463A071530ED.html\n",
    "│   │   └── 6B3BD97C-DEF2-49BB-B2B6-80F2CD53C4D3.html\n",
    "│   └── output/\n",
    "│       ├── index.json\n",
    "│       ├── results.csv\n",
    "│       └── wikipedia_index.json\n",
    "│\n",
    "├── indexer/\n",
    "│   ├── __pycache__/\n",
    "│   ├── __init__.py\n",
    "│   ├── extractor.py\n",
    "│   ├── indexer.py\n",
    "│   └── utils.py\n",
    "│\n",
    "├── processor/\n",
    "│   ├── __pycache__/\n",
    "│   ├── __init__.py\n",
    "│   ├── query_processor.py\n",
    "│   ├── similarity.py\n",
    "│   └── word2vec_search.py\n",
    "│\n",
    "├── queries.csv\n",
    "├── config.py\n",
    "├── main.py\n",
    "├── requirements.txt\n",
    "└── README.md\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
